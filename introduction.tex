\setcounter{theorem}{0}

\chapter{Introduction}

% Let us start with a brief historical walkthrough.
Statistics emerged more than a millennium ago, as frequency counting could be used to decipher encrypted messages.
Mathematical optimization started being formalized a few centuries ago, as
Newton and Gauss designed the first iterative methods to find an optimum, followed by \citet{cauchy1847methode}  who invented gradient descent.

Seventy years ago, scientists like Turing created digital computers and started dreaming of artificial intelligence: creating machines that could act and learn as humans do, or better.
A step towards this goal is enabling computers to interact with the world based on data: images, sound, text, or any arbitrary tabular data.
The most straightforward approach to give them this capability is to code explicit rules: if you receive this input, then you should do this.
Unfortunately, this approach, known as expert systems or symbolic artificial intelligence, does not scale with the complexity or the quantity of data:
imagine coding rules to identify a dog in an image given the raw string of one million pixels.
Almost as early as the first computer,  \citet{turing1950computing} formulated the idea of a program learning new rules from examples \citep{muggleton2014alan}.
Several years later, \citet{rosenblatt1957perceptron} introduced the perceptron, an early form of support vector machines doing exactly that.
This is the realm of machine learning where our story takes place.


Modern machine learning fits models to data points.
It is essentially a new field at the intersection of statistics and optimization, but some of its core ideas are not new.
They even predate the emergence of computers.
Legendre and Gauss already applied linear regression to predict planetary movements around 1800.
The long history of maximum likelihood can be summarized by the name of Fisher, who applied this principle to logistic regression in the 1930s \citep{stigler2007epic}.
Stochastic gradient descent \citep{robbins1951stochastic}, the workhorse of modern machine learning, was introduced as a root-finding algorithm.

The long history of statistics and optimization have intertwined to form machine learning as we know it today.
This thesis covers some of the interactions between these two fields.
Abstracting away, our contributions are  addressing three challenges
\begin{enumerate}
	\item training models faster
	\item learning causal features that generalize better
	\item characterizing the learning speed for a wide variety of models.
\end{enumerate}
By tapping into the connection between statistics and optimization, this thesis attempts to further progress on these three challenges.

\paragraph{Thesis Outline}
This thesis presents contributions spanning diverse parts of machine learning.
We provide \cref{ch:background} as a reference for readers that are unfamiliar with any of these parts.
In particular, we review the two learning setups that we are using: supervised learning and density estimation via maximum likelihood.
We then cover some fundamentals of convex optimization: SGD, variance reduction, and convex duality.
Finally, we introduce exponential families, graphical models and briefly introduce causal inference for the newcomers.

\cref{ch:sdca} is about training faster.
The goal is to train conditional random fields (CRF) as fast as possible.
CRF are probabilistic models for structured prediction that have long been notoriously hard to train.
For this purpose, we adapt SDCA, a well-known optimization algorithm.
On the way, we introduce a new sampling scheme for SDCA, and we prove an improved convergence rate.
Empirically, SDCA performs on par or better than other variance reduction algorithms.

\cref{ch:causal} is about optimization and causality.
It questions a specific assumption: we expect causal models to adapt faster to perturbations of the world. Is this true?
We answer this question for bivariate categorical and multivariate normal data by modeling perturbations as interventions in a ground truth causal model and modeling adaptation as optimization.

\cref{ch:map} is about optimization and elementary statistics.
While maximum likelihood estimates are used everywhere, they are most commonly used with exponential families where the solution has a closed-form.
A typical frequentist approach would estimate the average risk of this estimator, and a standard measure of risk would be the KL divergence between this estimator and the ground truth.
Nevertheless, we are not aware of any general results upper bounding this expected KL.
Surprisingly, this statistical problem is connected with the optimization of non-smooth losses.
We review the importance of this problem and showcase recent attempts at solving it.

Finally, the last chapter concludes the thesis by summarizing the contributions and outlining some future research directions in the context of this thesis.



\chapter{Background}
\label{ch:background}

In this section, we will review elementary building blocks that are pre-requisites to understand all three contributions.
First, we are going to review some fundamental principles of machine learning: empirical risk minimization and maximum likelihood estimation, for supervised learning \S\ref{ssec:supervised-learning} or density estimation \S\ref{ssec:density-estimation}.
Ultimately, we want to answer the following question: How can one minimize the empirical risk?
To do so, we will cover some parts of the vast topic that is optimization: stochastic gradient descent \S\ref{ssec:SGD}, variance reduction \S\ref{ssec:variance-reduction} and Fenchel duality \S\ref{ssec:fenchel-duality}.
Finally, we will describe some useful probabilistic models: exponential families \S\ref{ssec:exponential-families} (which are at the core of this thesis) probabilistic graphical models \S\ref{ssec:PGM}, and  structural causal models \S\ref{ssec:SCM}.

%Meanwhile a large body of work has grown around the idea of predicting the effect of an intervention in the world. This is the field of Causal Inference which we will cover in Section \ref{sec:causal_inference}.
%Machine learning typically frames the learning procedure as the minimization of some loss. How to efficiently minimize this loss over a high dimensional parameter and a large dataset is one of the grand challenge of machine learning. Optimization algorithms such as stochastic gradient descent have become the workhorse of modern machine learning. Characterizing the convergence behavior of these algorithms is critical to ensure that systems will learn. We will review some of these elements in Section \ref{sec:optimization}.


\section{Learning from Data}

\subsection{Supervised Learning}
\label{ssec:supervised-learning}
Let us assume we have $n$ data points $\cD = \paren{z_1, \dots , z_n }$, that decompose as $z_i = (x_i, y_i) \in \cX \times \cY$, where we call $x_i$ features, and $y_i$ labels.
We want to learn some rules to map newly observed features $x$ to their unobserved labels $y$.
The most common approach as of today is to define:
\begin{enumerate}
	\item a model $f_\theta: \cX \rightarrow \cY$ parametrized by some vector $\theta \in \real^d$. This model will implicitly contain all the rules that we are unable to explicitly write down. For $x$ and $y$ real vectors, the simplest instance of functions are linear models: $f_\theta(x) = \theta^T x$.
	\item a loss $\loss: \cY\times \cY \rightarrow \real$ that will tell use how well or how poorly we are doing. The lingua from statistical decision theory calls it \emph{risk function.} The simplest instance may be $\loss(y_1, y_2) = \norm{y_1 - y_2}^2$.
\end{enumerate}
Then we may learn the mapping by solving the following problem:
\alignn{
	\min_\theta  \sum_{i=1}^n \loss(y_i , f_\theta(x_i)) \; .
}
This problem simply minimizes the sum of the risk on all data points, hence the name: \emph{empirical risk minimization} or ERM to keep it short.

In this thesis, we are going to focus on a variant of ERM: \emph{maximum likelihood estimation}.
This special case happens when we assume that all data points were sampled independently and identically from some distribution $\ptrue(x, y)$ defined on $\cX\times \cY$.
Then $f_\theta(x_i)$ may return the log-probabilities (or the log-densities) of $y$ given $x_i$, e.g.
\alignn{
	f_\theta(x) = -\log \pmodel(y=\cdot \cond x) \in \real^{|\cY|} \; ,
}
and the loss may return the log-likelihood of $y_i$ given $x_i$, e.g.
\alignn{
	\loss(y_i , f_\theta(x_i)) = -\log \pmodel(y_i \cond x_i) \; .
}
Then, if $\theta^*$ solves the problem
\alignn{
	\theta^*
	&=\argmax_\theta \pmodel(y_1, \dots , y_n \cond x_1, \dots, x_n ) \\
	&= \argmin_\theta  \sum_{i=1}^n - \log \pmodel(y_i \cond x_i) \; ,
	\label{eq:supervised-MLE}
}
we will call it \emph{maximum likelihood estimate} or MLE for short.
Prior to supervised learning, statisticians have also studied this framework for density estimation.

\subsection{Density Estimation}
\label{ssec:density-estimation}
If we are not interested in a division of $z$ between features $x$ and labels $y$, we may still want to learn the probability distribution $\ptrue(z) = \ptrue(x, y)$. That is the realm of density estimation.
Now $\theta$ may parametrize a density model $z \mapsto \pmodel(z)$.
To solve this problem, maximizing the likelihood of the dataset is again an interesting approach:
\alignn{
	\theta^* = \argmin_\theta  \sum_{i=1}^n - \log \pmodel(x_i) \; .
	\label{eq:unsupervised-MLE}
}
In this case as well, $\theta^*$ is the \emph{maximum likelihood estimate} (MLE).
%Finally, to assess the quality of $\vp_{\theta^*}$, one can estimate the likelihood of a held-out test set.

\paragraph{Example 1: Isotropic Gaussian.}
If the data is made of floats $z\in\real^d$,
then we may chose to fit an isotropic Gaussian $\pmodel = \cN(\mu, \sigma^2 \mI)$.
The MLE simply estimates the noise and the variance of the data
\begin{align}
    \hat \mu = \inv{n} \sum_{i=1}^n x_i
    \quad \text{and} \quad
    \hat \sigma^2 = \inv{n} \sum_{i=1}^n \norm{x_i - \mu}^2 \; .
\end{align}

\paragraph{Example 2: Categorical.}
If the data is made of categories, e.g.,  $z \in \{1, \dots, K\}$, then the simplest model is the Categorical distribution $\pmodel = (p_1, \dots, p_K) \in \simplex_K$, which assigns a probability $p_k$ to each label $k$. We use $\simplex_K$ to denote the K-simplex, e.g., the set of real positive vectors of dimension $K$ that sum to 1:
\begin{align}
    \simplex_K = \left\{ \vp \in \real^K | \forall k, p_k \ge 0 ; \sum_{k=1}^K  p_k = 1 \right\} \; .
\end{align}
The maximum likelihood estimate of this model counts the occurrence of each label in the data and takes the empirical frequency of each label
\begin{align}
    \hat p_k  = \inv{n} \sum_{i=1}^n \ones\{x_i = k\}
\end{align}
where the indicator function $\ones\{ b\}$ is 1 if $b$ is true and 0 otherwise.

\paragraph{MAP.}
In some situations, we may take a Bayesian stance and assume the true parameter $\theta$ is itself a random variable sampled from a known \emph{prior} distribution $\ptrue(\theta)$.
Combining this prior with our model, we obtain a joint distribution on data and parameters
\alignn{
	\ptrue (z, \theta)
	:= \ptrue (z \cond \theta) \ptrue(z)
	:= \pmodel(z) \ptrue(\theta) \; .
}
By Bayes rule, we also obtain a posterior distribution on parameters
\alignn{
	\ptrue(\theta \cond z)
	= \frac{\ptrue(z, \theta)}{\ptrue(z)}
	\propto \pmodel(z) \ptrue(\theta)
}
The maximum a posteriori or MAP estimate is then defined as the parameter with maximal posterior density (or posterior mass for discrete parameters)
\alignn{
	\theta^*_\text{MAP}
	&:= \argmax \ptrue(\theta \cond z) \\
	&= \argmin -\log \pmodel(z) - \log \ptrue(\theta)
}
Letting go of the Bayesian perspective, this last minimization problem may be seen as an instance of regularized empirical risk minimization, where the negative log-likelihood of the prior $- \log \ptrue(\theta)$ plays the role of the regularizer.

\paragraph{Conjugate Priors.}
Given some models such as exponential families, there exist families of prior distributions $\cF = \bracket{\ptrue_\eta (\theta) \cond \eta}$
such that the posterior distribution also belongs to $\cF$,
e.g.  $\forall \eta, \exists \eta', \ptrue_\eta (\theta \cond z) = \ptrue_{\eta'}(\theta)$.
We refer to such families $\cF$ as \emph{conjugate priors}.
In our third contribution \S\ref{sec:background} we will cover in detail a generic instance of conjugate priors for exponential families.

\paragraph{Other models.}
Both Gaussians and categorical models belong to the general class of exponential families, which we will introduce in \S\ref{ssec:exponential-families}.
These families have a limited capacity: they cannot fit any distributions.
There exist much more powerful algorithms such as \emph{normalizing flows} \citep{rezende2015variational}, based on neural networks, which can model almost any smooth density by directly maximizing the likelihood of a dataset.

\paragraph{Beyond MLE.}
To learn from unlabeled data, there exist many competing approaches to MLE.
If we model the data with some unobserved variables, then we enter the realm of variational inference, with powerful algorithms such as variational auto-encoders \citep{kingma2013auto}.
If we are more interested in creating new realistic samples from $\ptrue(z)$, then adversarial training may be relevant \citep{goodfellow2014generative}.
Finally, for training large models, self-supervised learning has recently emerged as the leading set of techniques to learn powerful features, most notably for natural languages \citep{peters2018deep,devlin2018bert}.


\section{Convex Optimization}
Convex optimization the field of mathematics interested in solving problems of the form
\alignn{
	\min_{\theta \in \Theta} f(\theta)
}
where $f$ is a convex real valued function $f:\Theta \rightarrow \real$ called the loss or the \emph{objective function}, and $\Theta$ is a convex set called the \emph{constraint set}.
In this work, we always assume that the problem is unconstrained, i.e., $\Theta = \real^d$,
but with an objective function taking possibly infinite values, i.e., $f: \real^d \rightarrow \real \cup \{+\infty\}$.
The objective is thus implicitly defining a constraint set via its domain $\dom f = \bracket{ \theta \cond f(\theta) < +\infty }$.
For instance, we may encounter $\dom f = \cS_d^+$, the set of symmetric positive definite matrices of order $d$. Note that this set is open, and consequently, we cannot project onto it.
We also assume that $f$ is differentiable.

\paragraph{Gradient Descent.}
When $\Theta = \real^d$, and if we can compute derivatives of $f$, the most well-known algorithm to solve this problem is gradient descent.
Starting from a random point $\theta_0$, iteratively nudge the parameters in the direction opposite to the gradient of the loss, i.e.,
\alignn{
	\theta_{t+1}  = \theta_t - \lr_t \nabla f(\theta_t),
	\tag{GD}
}
where the hyper parameter $\lr_t$ is known as the \emph{step-size} or the learning rate.
The step-size may be constant, follow a predefined schedule, be found via a line-search, or be adaptive w.r.t. to the past trajectory.

\paragraph{Convergence Analysis.}
Gradient descent does not converge all the time.
We need assumptions on the objective function.
Perhaps the most common assumption is smoothness.
\begin{definition}[smoothness]
	A function $f:\Theta \rightarrow\real$ is said to be $L$-smooth if its gradient is $L$-Lipschitz, i.e.,
	\alignn{
		\forall \theta, \nu \in \Theta,
		\norm{\nabla f(\theta) - \nabla f(\nu)}
		\leq L \norm{\theta- \nu} \; .
	}
\end{definition}
If the loss  $f$ is convex and $L$-smooth then gradient descent with constant step-size $\lr_t = \inv{L}$ converges to a minimum $\theta^*$ at a rate $O(\inv{t})$ \citep[corollary 2.1.2]{nesterov2004Intro}.
Smoothness has a sibling assumption: strong-convexity.
\begin{definition}[strong-convexity]
	A function $f:\Theta \rightarrow\real$ is said to be $\mu$-strongly convex if its gradient verifies
	% \alignn{
	% 	\forall \theta, \nu \in \Theta,
	% 	\mu  \norm{\theta- \nu}
	% 	\leq \norm{\nabla f(\theta) - \nabla f(\nu)} \; .
	% }
	\alignn{
		\forall \theta, \nu \in \Theta,
		\mu  \norm{\theta- \nu}^2
		\leq \langle \nabla f(\theta) - \nabla f(\nu), \, \theta- \nu \rangle \; .
	}
\end{definition}
If $f$ is both $\mu$-smooth and $L$-strongly convex, then gradient descent with constant step-size $\lr_t = \frac{2}{\mu + L}$ converges at a \emph{linear rate} $O(e^{-\frac{t}{\kappa}})$ where $\kappa= \frac{L}{\mu} \geq 1$ is known as the condition number of the problem \citep[theorem 2.1.15]{nesterov2004Intro}.

\paragraph{Self-concordance.}
In contributions 2 and 3, we will face the log-likelihood of a multivariate normal variable.
This objective includes a term $g(\Lambda) = -\log \det \Lambda$ where $\Lambda\in \S_n^+$ is the positive definite precision matrix.
The objective $g$ shoots up to $+\infty$ when $\Lambda$ gets eigenvalues close from zero.
This means that its gradient is not a Lipschitz function.
In fact  $g$ is neither smooth nor strongly convex.
This kind of log-barrier objectives often comes up in interior point methods for solving constrained convex problems.
To analyze Newton's method applied on these objectives, a new assumption upper bounding the third derivative with the second derivative was introduced \citep{nemirovski1983problem}.
\begin{definition}[self-concordance]
	\citep[definition 4.1.1]{nesterov2004Intro}
	A convex function $f:\Theta \rightarrow\real$ is said to be self-concordant if
	\alignn{
		\forall \theta \in \Theta, \forall u,
		\nabla^3 f(\theta)[u,u,u] \leq 2 \nabla^2f(*\theta) [u,u]^{\frac{3}{2}}
	}
	where we evaluated the third-order tensor $\nabla^3 f(\theta)$in $u,u,u$.
\end{definition}
Under the self-concordance assumption, the analysis of Newton's method became lean and elegant. A fundamental consequence of self-concordance is that the objective can be sandwiched between quadratics in a clear neighborhood around its optimum. Additionally, many exponential families have self-concordant log-likelihood. We explore this fact in our third contribution.

\subsection{Stochastic Gradient Descent}
\label{ssec:SGD}
When $f$ has some structure, it is possible to design more efficient algorithms than gradient descent.
As seen in \cref{eq:supervised-MLE,eq:unsupervised-MLE}, machine learning is generally interested in minimizing an expected loss over a dataset, i.e.,
\begin{align}
	\min_\theta F(\theta):= \inv{n} \sum_{i=1}^n f(\theta, x_i) \; .
	\label{eq:finite-sum}
\end{align}
For instance, this loss may be the negative log-likelihood  $f(\theta, z_i) = - \log \pmodel(z_i)$.
The gradient of this empirical loss is the sum of gradients on each data points as follow,
\begin{align}
	\nabla F (\theta) = \inv{n}\sum_{i=1}^n \nabla_\theta f(\theta, x_i) \; .
\end{align}
Modern datasets are huge. They often contain millions, if not billions, of high dimensional data points such as images.
As a consequence exact minimization is no longer the bottleneck in learning \citep{bottou2008tradeoffs}.
Computing the exact gradient, a sum with a billion terms, is no longer affordable.
Instead, it is much more efficient to compute gradients for a few data point at a time, and take a step in their opposite direction in the hope of minimizing the loss
\begin{align}
	\theta_{t+1} = \theta_t - \lr_t \nabla_\theta f(\theta, x_i)
	\tag{SGD}
\end{align}
where $i$ is sampled uniformly from $\{1, \dots, n\}$.
This is \emph{stochastic gradient descent}\footnote{
	Contrary to gradient descent, SGD is not guaranteed to decrease the objective value at every step.
	As such, it is not a descent algorithm.
	We should rigorously call it the stochastic gradient method, but SGD has become the standard acronym in the community; therefore, we will stick with it.
}
(SGD).
It was first devised by \citet{robbins1951stochastic} to find the zeros of a stochastic function.


A special case happens when we sample each data point only once.
Then  SGD minimizes the true population risk
\begin{align}
	\min_\theta \ F(\theta):= \expect[x\sim \ptrue]{f(\theta, x)} \; .
	\label{eq:population-risk}
\end{align}
In our second contribution, we use this fact by interpreting convergence rates of SGD as a bound on the sample complexity of the model.

\paragraph{Convergence Analysis.}
Let us review a simplification of the modern convergence analysis from \citet{gower2019sgd}.
Assume that
\begin{itemize}
	\item $\forall x, f(\cdot, x)$ is $L$-smooth,
	\item$F$ is strongly convex, minimized by $\theta^*$,
	\item the gradient noise at the optimum is finite, i.e.,
	\[\sigma^2:= \expect[x\sim \ptrue]{\norm{\nabla f(\theta^*, x)}^2} < \infty. \]
\end{itemize}
Then iterates of SGD with constant step-size step size $\lr_t = \lr \in (0, \inv{2L}] $ verify \citep[theorem 3.1]{gower2019sgd}
\alignn{
	\expect{F(\theta_t)} - F(\theta^*)
	\leq \frac{L}{2} (1 - \lr \mu)^t \norm{\theta_0 - \theta^*}^2
	+ \lr \sigma^2 \frac{L}{\mu},
}
where the expectation is taken over the stochastic procedure.
In other words, SGD with constant step-size converges at a linear rate to a variance ball around the optimum, and the size of this variance ball is proportional to the step-size $\lr$, the gradient noise at the optimum $\sigma^2$ and the condition number $\frac{L}{\mu}$.
To overcome this variance ball issue, we may progressively decrease the learning rate $\lr_t \in O(\inv{t})$ to obtain a convergence rate $O(\inv{t})$ \citep[theorem 3.2]{gower2019sgd}.

\paragraph{SGD vs. GD.}
Recall that $n$ is the size of the dataset.
Each iteration of gradient descent has a compute cost of $O(n)$, whereas SGD has a constant cost of $O(1)$.
We see that even though each iteration of SGD is $n$ times more efficient than an iteration of full batch gradient descent, its overall convergence rate is $O(\inv{t})$, far worse than the linear rate of gradient descent $O(e^{-\frac{t}{\kappa}})$.
Finding an algorithm with a cheap $O(1)$ iteration cost and a linear convergence rate seemed impossible until the advent of SAG \citep{roux2012stochastic} and variance reduction techniques.

\subsection{Variance Reduction}
\label{ssec:variance-reduction}
Compared to the expected population risk in \cref{eq:population-risk}, the empirical risk \cref{eq:finite-sum} has a particular finite sum structure that SGD does not exploit.
Tapping into this structure, the breakthrough work of \citet{roux2012stochastic} designed and analyzed a stochastic algorithm with a  cheap $O(1)$ iteration cost and a linear convergence rate.
This algorithm is called stochastic averaged gradient or SAG.
Similar to SGD, at each step it sample a datapoint $x_i$ and computes its gradient $\nabla f(\theta_{t}, x_i)$.
The difference is that it estimates the true gradient thanks to past gradients of each individual data points $\nabla f(\theta_{t_i}, x_i)$ where $t_i$ is the last time that we sampled $x_i$.
Finally the update of SAG writes
\alignn{
	\theta_{t+1} = \theta_t - \frac{\lr_t}{n} \sum_i \nabla f(\theta_{t_i}, x_i) \;.
	\tag{SAG}
}
Following this path, \citet{defazio2014saga} introduced SAGA, a very similar algorithm with an unbiased gradient estimate allowing for more straightforward analysis.
Concurrently \citet{shalev-shwartz_stochastic_2013} analyzed SDCA, an algorithm maximizing the dual formulation (see \S\ref{ssec:fenchel-duality}), which enjoys the same cheap update cost and linear convergence rate.
In our first contribution, we improve upon SDCA and apply it to the challenging problem of conditional random fields (see \S\ref{ssec:PGM}).

Unfortunately, the memory footprint of SAG, SAGA or SDCA is $O(n d)$ in general (it can be reduced to $O(n)$ in many scenarios), which can quickly become prohibitive for large datasets or large models.
\citet{johnson2013accelerating} introduced stochastic variance reduced gradient (SVRG) to alleviate this issue.
Instead of storing all past gradients, SVRG stores one past iterate $\theta_T$ along with its full batch gradient $\nabla F(\theta_T)$, and it applies the update
\alignn{
	\theta_{t+1} = \theta_t - \lr_t (\nabla f(\theta_t) - \nabla f(\theta_T) + \nabla F(\theta_T))  \;.
	\tag{SVRG}
}
Thus SVRG only needs $O(d)$ memory, but it needs twice more compute than plain SGD. As such, the variance reduction technique is most amenable to optimizing large models such as neural networks.

\subsection{Fenchel Duality}
\label{ssec:fenchel-duality}
As previously mentioned, and fully explained in our first contribution, SDCA operates on the dual formulation of
\alignn{
	\min_\theta \inv{n} \sum_i f(y_i, \theta^\top x_i) + \frac{\lambda}{2} \norm{\theta}^2 \;.
}
However, what is this a dual formulation?

\paragraph{Convex Conjugates.}
To explain Fenchel duality properly, we need to introduce the convex conjugate of a function.
\begin{definition}[convex conjugate]
	The convex conjugate of a function $f:\real^d \rightarrow \real\cup{+\infty}$ is defined by the pointwise formula
	\alignn{
		f^*(y):= \max_x \lin{y, x} - f(x)
	}
\end{definition}
This transformation is a ubiquitous concept throughout Science.
In thermodynamics and classical mechanics, it appears as the Legendre transform (a special case). In convex optimization and machine learning, we call it the Fenchel conjugate or the convex conjugate.
At first sight, this definition seems arbitrary, but it admits geometrical interpretations along with many properties that make it a helpful tool.
The author of this thesis produced several interactive tools to grasp a better understanding of convex conjugates: \href{https://remilepriol.github.io/dualityviz/}{DualityViz} and \href{https://remilepriol.github.io/dualityviz/dual_snakes.html}{Dual Snakes}.

One of the most interesting properties of convex conjugation is that for convex functions, the convex conjugate of the conjugate is equal to the function itself, i.e.,
\alignn{f^{**} = f \; .}

\paragraph{Fenchel Dual.}
Assume we want to solve a composite minimization problem
\alignn{
	\min_x f(x) + g(\mA x)
	\label{eq:composite-minimization}
}
where $f:\cX \rightarrow\real$ and $g:\cY \rightarrow \real$ are convex functions and $\mA: \cX \rightarrow \cY$ is a linear operator.
Under mild assumptions, this problem can be equivalently expressed with the convex conjugates of $f$ and $g$
\alignn{
	\min_x f(x) + g(\mA x)
	&= \min_x \max_y f(x) + \lin{Ax, y} - g^*(y) \\
	&\geq \max_y \min_x f(x) + \lin{x, A^\top y} - g^*(y) \\
	&= \max_y - f^*(-A^\top y) - g^*(y) \; .
}
This last line is known as the Fenchel dual of problem~\eqref{eq:composite-minimization}.
We inverted min and max between the first and second line to reach it.
Fenchel's duality theorem states sufficient conditions for this inequality to be an equality, in which case we say that strong duality holds.
Fenchel duality is equivalent to Lagrange duality, but Fenchel's is more convenient for unconstrained problems or problems where the constraints are implicitly defined in the objective, whereas Lagrange's is more convenient for explicitly defined constraints.

SDCA and many other optimization algorithms directly store and update the dual variable $y$.
It is well defined for generalized linear models, e.g., models defined with the exponential family.

\section{Probabilistic Models}
\subsection{Exponential Families}
\label{ssec:exponential-families}
Exponential families are among the simplest parametric models of distributions.
To define an exponential family, take a variable $x$ in $\cX$ equipped with the base measure $\nu$.
Then extract a sufficient statistic $T(x) \in \real^d$.
Then take the inner product between some parameter $\nat$ and $T(x)$.
This inner product may be negative, so to ensure it is positive, take its exponential $e^{ \lin{\E[T(x)] , \nat} }$.
The mass (for discrete random variables) or the density (for continuous random variables) with respect to $\nu$ is then defined to be proportional to this exponential
\alignn{
	\pmodel(x) \propto e^{ \lin{\E[T(x)] , \nat} } \nu(x) \; .
}
The logarithm of the normalization constant is known as the log-partition function
\alignn{
	A(\nat):= \log \int e^{\langle \nat, T(x) \rangle} \nu(dx) \;.
}
The equation for the negative log-likelihood finally reads
\alignn{
	f(\nat):= \E[-\log p_\nat(X)] = A(\nat) - \lin{\E[T(X)] , \nat} \; .
}
Remark that $f$ is convex, and it can be seen as a linear modification of the log-partition function $A$, which contains all the complexity.
We provide more properties of these families in our third contribution.

We can always define $z=T(x)$, with $\mu$ the proper push forward modification of $\nu$, in which case we say that $Z$ belongs to the \emph{natural exponential family} on $\mu$.

The most common parametric distributions are exponential families:
Categorical, Gaussians, Gamma, Wishart, Dirichlet, \textit{etc.}
A remarkable exception is the non-central Laplace $p(x) \propto e^{\abs{x-\mu}}$ which cannot be expressed in this form.


\paragraph{GLM.}
Generalized linear models (GLM) are a powerful tool in supervised learning.
Taking features $x$ and labels $y$, a GLM models the conditional distribution $\ptrue(y \cond x)$ with an exponential family whose parameter is a linear function of $x$.
For simplicity, we consider the natural exponential family $T(y) = y$.
The model writes
\alignn{
	\pmodel ( y \cond x) = \exp( y^\top \vtheta x - A(\vtheta x) )
}
where $\vtheta$ is a matrix of size $\dim(y) \times \dim(x)$.

In our first contribution, we study an algorithm for training a GLM for categorical distributions, e.g., logistic regression, with the number of categories growing exponentially with the input size.
For this purpose, we use independence assumptions that are formalized by probabilistic graphical models.

\subsection{Probabilistic Graphical Models}
\label{ssec:PGM}

One of the most useful properties we can model about the natural distribution $\ptrue(x)$ is the notion of (conditional) independence between variables. For instance, in a simple video game, two stacks of frames are often independent, given the stack of frames in between them. This kind of independence statements can be specified with graphs thanks to probabilistic graphical models -- see \citet{pearl1988probabilistic} for an historical reference, or \citet{wainwright2008graphical} or  \citet{koller2009PGM} for a more recent review.

We start by presenting undirected graphical models.
GLMs associated with undirected graphical models are known as the conditional random field (CRF), and they are at the core of our first contribution.
Then we introduce directed graphical models, which are necessary to understand structural causal models, and our second contribution.

\subsubsection{Undirected Graphical Models, a.k.a. Markov Random Fields}
Let $\cG$ be an undirected graph defined by
its vertices $\cV = \bracket{1,\dots,d}$
and its edges $(i, j) \in \cE$.
\begin{definition}[clique]
	The set $C = \bracket{v_1, \dots, v_k}$ is said to be
	a clique of $\cG$ if and only if it forms
	a complete graph, e.g.
	$\forall i \neq j, (v_i, v_j) \in \cE$.
\end{definition}
\begin{definition}[maximal clique]
	A clique $C$ is maximal if it is not contained in any clique, e.g.
	$\forall C', (C\subset C' \implies C'$ is not a clique$)$.
\end{definition}
We name $\cC$ the set of maximal cliques of $\cG$.
We are now ready to define the independence statement.
\begin{definition}
	A distribution $\ptrue$ is said to factor along $\cG$ if and only if its density verifies
	\alignn{
		\ptrue(x) = \prod_{C\in \cC} \psi_C (x_C)
	}
	where $x_C$ is a vector containing the rows of $x$ indexed by $i \in C$, and $\psi_C: \real^{\abs{C}} \rightarrow \real$ are real-valued functions of $\abs{C}$ elements.
	We refer to $\psi_C$ as the potential of the clique $C$.
\end{definition}


\paragraph{Exponential Graphical Model.}
If $\pmodel$ belongs to the exponential family, then a sufficient condition for $\pmodel$  to factor along $\cG$ is for its sufficient statistic to decompose along with the cliques of $\cG$, e.g.
\alignn{
	T(x) &= \sum_{C\in\cC} T_C(x_C) \\
	\implies
	\pmodel(x) &\propto \prod_{C\in \cC} e^{\lin{T_C(x_C) , \nat}} \; .
}
We exploit this fact in our first contribution.


\subsubsection{Directed Graphical Models, a.k.a. Bayesian Networks}
Directed probabilistic graphical models are also known as Bayesian Networks since \citet{pearl1985bayesian} coined this term.
They are perhaps simpler to understand than undirected graphical models, but they are not easier to deal with.

Suppose we observe a random variable $X=(X_1, \dots, X_d) \in \real^d$ with probability law $\ptrue(X)$.
We are also given a Directed Acyclic Graph (DAG) $\cG$ with vertex $\cV=\{1, \dots, d \}$ and edges $\cE$ .
We denote $\parents(i)$ the parents of node $i$.
This is the empty set if $i$ has no parents.
\begin{definition}
	We say that $\ptrue$ factorizes along $\cG$ iff
	\begin{align}
		\ptrue(X) = \prod_{i=1}^d \ptrue(X_i | X_{\parents(i)}) \; .
	\end{align}
\end{definition}
In other words, the only conditional dependencies of $\ptrue$ are indicated by the edges of the graph $G$. The fewer edges in $G$, the more we know about $X$. In fact, if we know nothing about $\ptrue$, we still know that we can write it as
\begin{align}
	\ptrue(X) = \prod_{i=1}^d \ptrue(X_i | X_{<i} )
\end{align}
by definition of conditional probability -- modulo some positivity constraints. Consequently, a useful graph should have only a few edges, or equivalently a low degree.

\paragraph{Structure Learning.}
In unsupervised learning, either we posit that the data factorizes along with a graph and exploit this information to learn a density model $\pmodel$ with fewer data. Either we set the goal of discovering these conditional independence structures. This goal is known as structure learning. Current solutions to this problem fall into two categories
\begin{enumerate}
	\item Explicitly find out conditional independences with statistical testing and build the graph from there.
	\item Use a scoring function to explore all possible graphs and keep the one with the highest score. The scoring function is often designed as the posterior probability of the structure given the data.
\end{enumerate}

Directed graphical models have proven helpful in many modeling areas. However, they alone cannot predict what will happen if one of the variables is affected by some external stimuli. That is the topic of causal inference.

\subsection{Causal Inference}
\label{ssec:SCM}

Causal inference use directed graphical models to predict the effect of interventions in the world. We will now introduce two key elements of this theory: do-calculus and structural causal models.

\subsubsection{Do-calculus}
Assume we have data for kidney stone treatments performed in one hospital.
For each patient, we know the treatment they received $X$, the outcome $Y$ -- did they successfully heal? -- and the size of the stones they found during the surgery $Z$.
A new patient arrives. We have to recommend the treatment that will maximize their chance of recovery.
How should we process the data to make this decision?

This classic story is an instance of Simpson's paradox.
The straightforward solution would be to recommend the treatment with the highest success rate in this example.
However, it so happens that the treatment received by past patients was picked based on their symptoms, which were themselves a function of the stone sizes.
In this example, the stone size is a \textit{confounder} that affects both the treatment and the outcome.
First, one should partition based on $Z$ the data before aggregating the success rates. But why is that, and how to formalize that? The answer lies in the work of Judea Pearl \citep{pearl2009causality} and other statisticians. It can be formalized with the help of graphical models such as Figure \ref{fig:simpsons_paradox}.

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{scope}[every node/.style={circle,thick,draw}]
        \node (X) at (-2,0) {$X$};
        \node (Y) at (2,0) {$Y$};
        \node (Z) at (0,3) {$Z$};
    \end{scope}

    \begin{scope}[>={Stealth[black]},
                  every edge/.style={draw=black,very thick}]
        \path [->] (X) edge (Y);
        \path [->] (Z) edge (Y);
        \path [->] (Z) edge (X);
    \end{scope}
    \end{tikzpicture}
    \caption{The graph of causal relationships between treatments $X$, outcome $Y$ and stone size $Z$. $Z$ is a cause of both $X$ and $Y$, which makes it a confounder.}
    \label{fig:simpsons_paradox}
\end{figure}

The question we asked is an \textit{interventional question}: what will happen \textit{if we assign $X=x$?} This action effectively removes the observed statistical dependency between $X$ and $Z$. The outcome distribution of this action should not be computed as the simple conditional probability $P(Y|X=x)$, but as another quantity that we will denote $P(Y| \Do(x))$. The gold standard to estimate this quantity would be to perform a \textit{randomized control trial}, where we blindly and randomly assign treatments to incoming patients, then observe and report success rate. However, we want to exploit the observed data to estimate this quantity. That is where the \textit{do-calculus} comes into play. It is a set of rules based on graphs that transform do-statements such as $P(Y| \Do(x))$ into an equation written in terms of observed probabilities.
In the kidney stone example, we can estimate $P(Y| \Do(x))$ from observational data thanks to the \textit{backdoor adjustment formula}
\begin{align}
	P(Y|\Do(x)) = \sum_z P(Y | x, z) P(z)
	\neq \sum_z P(Y | x, z) P(z|x) = P(Y | x) \; .
	\label{eq:backdoor_adjustment}
\end{align}

What is critical here is that $Z$ is a cause of $X$. If instead $X$ caused $Z$ then causal effect and conditional would be equal $P(Y|\Do(x)) = P(Y | x)$. Yet from a Bayesian network perspective, both arrow directions make a complete graph, which encodes the same absence of conditional independence. In other words, a causal graphical model encodes strictly more information than a Bayesian network.

The backdoor adjustment formula is the most famous instance of do-calculus, but more complex rules exist for complex graphs with both observed and unobserved variables. Quite recently, \citet{huang2012pearl} proved that these rules are complete, meaning that if a do-statement can be expressed in terms of observed probabilities, then one will be able to find the right formula by applying these rules.



\subsubsection{Structural Causal Models}

Thanks to the rules of do-calculus, knowing the causal graph can be handy. So far, we have talked about this in a non-parametric setting, assuming we have direct access to the observed conditional probabilities $P(Y|X, Z)$. In reality, we need to parametrize these mechanisms. This is what a Structural Causal Model (SCM) is for. It describes a causal model by a set of unobserved independent exogenous noise variables $U_1, \dots, U_d$, and a set of functions $f_1, \dots, f_d$ such that
\begin{align}
	X_i = f_i(X_{\parents(i)}, U_i), \forall i \;.
	\label{eq:scm}
\end{align}
Given a DAG $\cG$, these functions, and distributions for the exogenous noise, one can sample a vector $X$ by sampling the noises and applying these functions in a topological order of $\cG$.

Among other things, SCMs are helpful to answer \textit{counterfactual questions}: what would have happened if I had given the other treatment to this patient? Counterfactuals are a major topic in the causality community, but they are not relevant to this thesis, so we will not cover this theory.

While the formalism of \eqref{eq:scm} may seem trivial at first, it becomes useful when one starts thinking about causal structure discovery.
If we assume a parametric form for the functions, then the graphical structure can become identifiable, meaning that only one graph could have generated the observed data. One such example is if we assume $f_i$ are linear and noises are non-Gaussian.
However, the interest of these identifiability results is limited because, in general, we have no guarantee on the shape of the function that generated the data.

The SCM formalism enables us to think about a much deeper hypothesis: \textbf{Independent Causal Mechanisms}. This hypothesis postulates that knowing something about one mechanism does not provide any information about the others. That can be formalized by various means. One of them is Algorithmic Information Theory: the Kolmogorov Complexity of the set $\{ f_1, \dots, f_d \}$
\footnote{We do not include the exogenous noise distributions for simplicity.}
is on the same order of magnitude as the sum of the Kolmogorov Complexity of each function taken independently.
Using this independence insight, one can devise algorithms that aim to find the data's causal structure. See \citet{peters2017elements} for a book on this topic.

Our second contribution addresses an idea to discover causal structure from interventional data, e.g., data coming from (possibly unknown) interventions.
We have now provided all the key elements to understand this thesis.
Let us emphasize that causal inference and causal discovery are taking more and more space in machine learning.
We refer the reader to \citep{scholkopf2019causality} for a modern review of the literature.

