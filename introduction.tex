\setcounter{theorem}{0}

\chapter{Introduction}

\paragraph{Context: optimization and statistics are intertwined}
wth a chronological perspective.

Statistics for policies and for scientific experiments.

in parallel optimization in mathematics (cf gradient descent, cauchy)

Then computers and artificial intelligence thanks to expert systems. 

Then all three came together in machine learning, for the optimization of statistical models, cf perceptron.

\paragraph{challenges of this field}
we want to train models faster. 
we want to learn causal features.
we want to quantify the learning speed for a wide variety of models.

\paragraph{Thesis Outline}




A primary goal of artificial intelligence is to enable computers to interact with the world based on data they may receive: images, sound, text or any arbitrary tabular data. 
The most straightforward approach to give them this capability is to code explicit rules : if you receive this input, then you should do this. 
Unfortunately, this approach, known as expert systems or good-old-fashioned-artificial-intelligence, does not scale with the complexity or the quantity of data:
imagine coding rules to identify a dog in an image given the raw string of one million pixels. 
Almost as early as the first computer was created \citep{rosenblatt1957perceptron}, computer scientists have engaged in another route to solve this problem : learning these rules from data, e.g. learning to recognize dogs from thousands or millions of dog and non-dog pictures. 
This is the vast field of \textit{Machine Learning}. 
Now let us first review the most common machine learning setting : supervised learning.


\chapter{Background}

In this section, we are going to review elementary building blocks that are pre-requisites to understand all three contributions.
First we are going to review some fundamental principles of machine learning : empirical risk minimization and maximum likelihood estimation, for supervised learning \S\ref{ssec:supervised-learning} or for density estimation \S\ref{ssec:density-estimation}.
How can one minimize the empirical risk ?  
To answer this question, we will cover some parts of the vast topic that is optimization : stochastic gradient descent \S\ref{ssec:SGD}, variance reduction \S\ref{ssec:variance-reduction} and Fenchel duality \S\ref{ssec:fenchel-duality}.
Finally, we will describe some useful probabilistic models : exponential families \S\ref{ssec:exponential-families}, which are at the core of this thesis, probabilistic graphical models \S\ref{ssec:PGM}, and  structural causal models \S\ref{ssec:SCM}.

%Meanwhile a large body of work has grown around the idea of predicting the effect of an intervention in the world. This is the field of Causal Inference which we will cover in Section \ref{sec:causal_inference}. 
%Machine learning typically frames the learning procedure as the minimization of some loss. How to efficiently minimize this loss over a high dimensional parameter and a large dataset is one of the grand challenge of machine learning. Optimization algorithms such as stochastic gradient descent have become the workhorse of modern machine learning. Characterizing the convergence behavior of these algorithms is critical to ensure that systems will learn. We will review some of these elements in Section \ref{sec:optimization}.


\section{Learning from Data}

\subsection{Supervised Learning}
\label{ssec:supervised-learning}
Let us assume we have $n$ data points $\cD = \paren{z_1, \dots , z_n }$, that decompose as $z_i = (x_i, y_i) \in \cX \times \cY$, where we call $x_i$ features, and $y_i$ labels.
We want to learn some rules to map newly observed features $x$ to their unobserved labels $y$.  
The most common approach as of today is to define :
\begin{enumerate}
	\item a model $f_\theta : \cX \rightarrow \cY$ parametrized by some vector $\theta \in \real^d$. This model will implicitly contain all the rules that we are unable to explicitly write down. For $x$ and $y$ real vectors, the simplest instance of functions are linear models : $f_\theta(x) = \theta x$.
	\item a loss $\loss : \cY\times \cY \rightarrow \real$ that will tell use how well or how poorly we are doing. The lingua from statistical decision theory calls it \emph{risk function.} The simplest instance may be $\loss(y_1, y_2) = \norm{y_1 - y_2}^2$.
\end{enumerate}
Then we may learn the mapping by solving the following problem :
\alignn{
	\min_\theta  \sum_{i=1}^n \loss(y_i , f_\theta(x_i)) \; .
}
This problem simply minimizes the sum of the risk on all data points, hence the name : \emph{empirical risk minimization} or ERM to keep it short.

In this thesis, we are going to focus on a variant of ERM : \emph{maximum likelihood estimation}.
This special case happens when we assume that all data points were sampled independently  and identically from some distribution $\ptrue(x, y)$ defined on $\cX\times \cY$.
Then $f_\theta(x_i)$ may return the log-probabilities (or the log-densities) of $y$ given $x_i$, e.g. 
\alignn{
	f_\theta(x) = -\log \pmodel(y=\cdot \cond x) \in \real^\cY \; ,
}
and the loss may return the log-likelihood of $y_i$ given $x_i$, e.g.
\alignn{
	\loss(y_i , f_\theta(x_i)) = -\log \pmodel(y_i \cond x_i) \; .
}
Then, if $\theta^*$ solves the problem
\alignn{
	\theta^*
	&=\argmax_\theta \pmodel(y_1, \dots , y_n \cond x_1, \dots, x_n ) \\
	&= \argmin_\theta  \sum_{i=1}^n - \log \pmodel(y_i \cond x_i) \; ,
	\label{eq:supervised-MLE}
}
we will call it \emph{maximum likelihood estimate} or MLE for short.
Prior to supervised learning, this framework has also been largely studied by statisticians for density estimation.

\subsection{Density Estimation}
\label{ssec:density-estimation}
If we are not interested in a division of $z$ between features $x$ and labels $y$, we may still want to learn the probability distribution $\ptrue(z) = \ptrue(x, y)$. This is the realm of density estimation. 
Now $\theta$ may parametrize a density model $z \mapsto \pmodel(z)$. 
To solve this problem, maximizing the likelihood of the dataset is again an interesting approach :
\alignn{
	\theta^* = \argmin_\theta  \sum_{i=1}^n - \log \pmodel(x_i) \; .
	\label{eq:unsupervised-MLE}
}
In this case as well, $\theta^*$ is the \emph{maximum likelihood estimate} (MLE).
%Finally, to assess the quality of $\vp_{\theta^*}$, one can estimate the likelihood of a held-out test set. 

\paragraph{Example 1: Isotropic Gaussian.}
If the data is made of floats $z\in\real^d$, 
then we may chose to fit an isotropic Gaussian $\pmodel = \cN(\mu, \sigma^2 \mI)$.
The MLE simply estimates the noise and the variance of the data
\begin{align}
    \hat \mu = \inv{n} \sum_{i=1}^n x_i 
    \quad \text{and} \quad 
    \hat \sigma^2 = \inv{n} \sum_{i=1}^n \norm{x_i - \mu}^2 \; .
\end{align}

\paragraph{Example 2: Categorical.}
If the data is made of categories -- e.g.  $z \in \{1, \dots, K\}$ -- then the simplest model is the Categorical distribution $\pmodel = (p_1, \dots, p_K) \in \simplex_K$, which assigns a probability $p_k$ to each label $k$. We use $\simplex_K$ to denote the K-simplex -- e.g. the set of real positive vectors of dimension $K$ that sum to 1
\begin{align}
    \simplex_K = \left\{ \vp \in \real^K | \forall k, p_k \ge 0 ; \sum_{k=1}^K  p_k = 1 \right\} \; .
\end{align}
The maximum likelihood estimate of this model counts the occurrence of each label in the data and takes the empirical frequency of each label
\begin{align}
    \hat p_k  = \inv{n} \sum_{i=1}^n \ones\{x_i = k\}
\end{align}
where the indicator function $\ones\{ b\}$ is 1 if $b$ is true and 0 otherwise.

\paragraph{MAP.}
In some occasions, we may take a Bayesian stance and assume the true parameter $\theta$ is itself a random variable sampled from a known \emph{prior} distribution $\ptrue(\theta)$. 
Combining this prior with our model, we obtain a joint distribution on data and parameters 
\alignn{
	\ptrue (z, \theta) 
	:= \ptrue (z \cond \theta) \ptrue(z) 
	:= \pmodel(z) \ptrue(\theta) \; .
}
By Bayes rule, we also obtain a posterior distribution on parameters 
\alignn{
	\ptrue(\theta \cond z)
	= \frac{\ptrue(z, \theta)}{\ptrue(z)}
	\propto \pmodel(z) \ptrue(\theta)
}
The maximum a posteriori or MAP estimate is then defined as the parameter with maximal posterior density (or posterior mass for discrete parameters)
\alignn{
	\theta^*_\text{MAP} 
	&:= \argmax \ptrue(\theta \cond z) \\
	&= \argmin -\log \pmodel(z) - \log \ptrue(\theta)
}
Letting go of the Bayesian perspective, this last minimization problem may be seen as an instance of regularized empirical risk minimization, where the negative log-likelihood of the prior $- \log \ptrue(\theta)$ plays the role of the regularizer.

\paragraph{Conjugate Priors.}
Given some models such as exponential families, there exist families of prior distributions $\cF = \bracket{\ptrue_\eta (\theta) \cond \eta}$ 
such that the posterior distribution also belongs to $\cF$, 
e.g.  $\forall \eta, \exists \eta', \ptrue_\eta (\theta \cond z) = \ptrue_{\eta'}(\theta)$.
We refer to such families $\cF$ as \emph{conjugate priors}.
In our third contribution \S\ref{sec:background} we will cover in detail a generic instance of conjugate priors for exponential families.

\paragraph{Other models.}
Both Gaussians and categorical models belong to the general class of exponential families which we will introduce in \S\ref{ssec:exponential-families}.
These families have a limited capacity : they cannot fit any distributions.
There exist much more powerful algorithms such as \emph{normalizing flows} \citep{rezende2015variational}, based on neural networks, which can model almost any smooth density by directly maximizing the likelihood of a dataset.

\paragraph{Beyond MLE.} 
To learn from unlabeled data, there exist many competing approaches to MLE. 
If we model the data with some unobserved variables, then we enter the realm of variational inference, with powerful algorithms such as variational auto-encoders \citep{kingma2013auto}. 
If we are more interested in creating new realistic samples from $\ptrue(z)$, then adversarial training may be relevant \citep{goodfellow2014generative}.
Finally for training large models, self-supervised learning has recently emerged as the leading set of techniques to learn powerful features, most importantly for natural languages \citep{peters2018deep,devlin2018bert}.


\section{Convex Optimization}
Convex optimization the field of mathematics interested in solving problems of the form
\alignn{
	\min_{\theta \in \Theta} f(\theta)
}
where $f$ is a convex real valued function $f:\Theta \rightarrow \real$ called the loss or the \emph{objective function}, and $\Theta$ is a convex set called the \emph{constraint set}.
In this work, we always assume that the problem is unconstrained, e.g. $\Theta = \real^d$, 
but with an objective function taking possibly infinite values, e.g. $f : \real^d \rightarrow \real \cup \{+\infty\}$. 
The objective is thus implicitly defining a constraint set via its domain $\dom f = \bracket{ \theta \cond f(\theta) < +\infty }$.
For instance, we may encounter $\dom f = \cS_n^+$, the set of symmetric positive definite matrices of order $n$. Note that this set is open, and consequently we cannot project onto it.
We also assume that $f$ is differentiable.

\paragraph{Gradient Descent.}
When $\Theta = \real^d$ and we are able to compute  derivatives of $f$, the most well-known algorithm to solve this problem is gradient descent. 
Starting from a random point $\theta_0$, iteratively nudge the parameters in the direction opposite to the gradient of the loss
\alignn{
	\theta_{t+1}  = \theta_t - \lr_t \nabla f(\theta_t)
	\tag{GD}
}
where the hyper parameter $\lr_t$ is known as the \emph{step-size} or the learning rate. 
The step-size may be constant, follow a predefined schedule, be found via a line-search, or adapt to the past trajectory.

\paragraph{Convergence Analysis.}
Gradients descent does not converge all the time. 
We need assumptions on the objective function.
Perhaps the most common assumption is smoothness.
\begin{definition}[smoothness]
	A function $f:\Theta \rightarrow\real$ is said to be $L$-smooth if its gradient is $L$-Lipschitz, e.g.
	\alignn{
		\forall \theta, \nu \in \Theta,
		\norm{\nabla f(\theta) - \nabla f(\nu)} 
		\leq L \norm{\theta- \nu} \; .
	}
\end{definition}
If the loss  $f$ is convex and $L$-smooth then gradient descent with constant step-size $\lr_t = \inv{L}$ converges to a minimum $\theta^*$ at a rate $O(\inv{t})$ \citep[corollary 2.1.2]{nesterov2004Intro}. 
Smoothness has a sibling assumption : strong-convexity.
\begin{definition}[strong-convexity]
	A function $f:\Theta \rightarrow\real$ is said to be $\mu$-strongly convex if its gradient verifies
	\alignn{
		\forall \theta, \nu \in \Theta,
		\mu  \norm{\theta- \nu} 
		\leq \norm{\nabla f(\theta) - \nabla f(\nu)} \; .
	}
\end{definition}
If $f$ is both $\mu$-smooth and $L$-strongly convex, then gradient descent with constant step-size $\lr_t = \frac{2}{\mu + L}$ converges at a \emph{linear rate} $O(e^{-\frac{t}{\kappa}})$ where $\kappa= \frac{L}{\mu} \geq 1$ is known as the condition number of the problem \citep[theorem 2.1.15]{nesterov2004Intro}.

\paragraph{Self-concordance.}
In contributions 2 and 3, we will face the log-likelihood of a multivariate normal variable. 
This objective includes a term $g(\Lambda) = -\log \det \Lambda$ where $\Lambda\in \S_n^+$ is the positive definite precision matrix. 
The objective $g$ shoots up to $+\infty$ when $\Lambda$ gets eigenvalues close from zero. 
This means that its gradient is not a Lipschitz function.
In fact  $g$ is neither smooth nor strongly convex.
This kind of log-barrier objectives often comes up in interior point methods for solving constrained convex problems.
To analyze Newton's method applied on these objectives, a new assumption upper bounding the third derivative with the second derivative was introduced \citep{nemirovski1983problem}. 
\begin{definition}[self-concordance]
	\citep[definition 4.1.1]{nesterov2004Intro}
	A convex function $f:\Theta \rightarrow\real$ is said to be self-concordant if
	\alignn{
		\forall \theta \in \Theta, \forall u, 
		\nabla^3 f(\theta)[u,u,u] \leq 2 \nabla^2f(*\theta) [u,u]^{\frac{3}{2}}
	}
	where we evaluated the third order tensor $\nabla^3 f(\theta)$in $u,u,u$.
\end{definition}
Under self-concordance assumption, the analysis of Newton's method became lean and elegant. A fundamental consequence of self-concordance is that the objective can be sandwiched between quadratics in a clear neighborhood around its optimum. Additionally, many exponential families have self-concordant log-likelihood. We explore this fact in our third contribution.

\subsection{Stochastic Gradient Descent}
\label{ssec:SGD}
When $f$ has some structure, it is possible to design more efficient algorithms than gradient descent.
As seen in \cref{eq:supervised-MLE,eq:unsupervised-MLE}, machine learning is generally interested in minimizing an expected loss over a dataset
\begin{align}
    \min_\theta F(\theta) := \inv{n} \sum_{i=1}^n f(\theta, x_i) \; .
    \label{eq:finite-sum}
\end{align}
For instance, this loss may be the negative log-likelihood  $f(\theta, z_i) = - \log \pmodel(z_i)$. 
The gradient of this empirical loss is the sum of gradients on each data points
\begin{align}
    \nabla F (\theta) = \inv{n}\sum_{i=1}^n \nabla_\theta f(\theta, x_i) \; .
\end{align}
Modern datasets are huge. They often contain millions, if not billions, of high dimensional data points such as images. 
As a consequence exact minimization is no longer the bottleneck in learning \citep{bottou2008tradeoffs}. 
Computing the exact gradient, a sum with a billion terms, is no longer affordable.
Instead, it is much more efficient to compute gradients for a few data point at a time, and take a step in their opposite direction in the hope of minimizing the loss
\begin{align}
    \theta_{t+1} = \theta_t - \lr_t \nabla_\theta f(\theta, x_i)
    \tag{SGD}
\end{align}
where $i$ is sampled uniformly from $\{1, \dots, n\}$.
This is \emph{stochastic gradient descent}\footnote{
	Contrary to gradient descent, SGD is not guaranteed to decrease the objective value at every step. 
	As such, it is not a descent algorithm. 
	We should rigorously call it stochastic gradient method, but SGD has become the standard acronym in the community so we will stick with it.
}
(SGD).
It was first devised by \citet{robbins1951stochastic} to find the zeros of a stochastic function.


A special case happens when we sample each data point only once.
Then  SGD minimizes the true population risk 
\begin{align}
    \min_\theta \ F(\theta) := \expect[x\sim \ptrue]{f(\theta, x)} \; .
    \label{eq:population-risk}
\end{align}
In our second contribution, we use this fact by interpreting convergence rates of SGD as a bound on the sample complexity of the model. 

\paragraph{Convergence Analysis.}
Let us review a simplification of the modern convergence analysis from \citet{gower2019sgd}.
Assume that
\begin{itemize}
    \item $\forall x, f(\cdot, x)$ is $L$-smooth,
    \item$F$ is strongly convex, minimized by $\theta^*$,
    \item the gradient noise at the optimum 	is finite, e.g. $\sigma^2 := \expect[x\sim \ptrue]{\norm{\nabla f(\theta^*, x)}^2} < \infty$.
\end{itemize}
Then iterates of SGD with constant step-size step size $\lr_t = \lr \in (0, \inv{2L}] $ verify \citep[theorem 3.1]{gower2019sgd}
\alignn{
	\expect{F(\theta_t)} - F(\theta^*)
	\leq \frac{L}{2} (1 - \lr \mu)^t \norm{\theta_0 - \theta^*}^2 
	+ \lr \sigma^2 \frac{L}{\mu}
}
where the expectation is taken over the stochastic procedure.
In other words, SGD with constant step-size converges at a linear rate to a variance ball around the optimum, and the size of this variance ball is proportional to the step-size $\lr$, the gradient noise at the optimum $\sigma^2$ and the condition number $\frac{L}{\mu}$. 
To overcome this variance ball issue, we may progressively decrease the learning rate $\lr_t \in O(\inv{t})$ to obtain a convergence rate $O(\inv{t})$ \citep[theorem 3.2]{gower2019sgd}.

\paragraph{SGD vs GD.}
Recall that $n$ is the size of the dataset.
Each iteration of gradient descent has a compute cost of $O(n)$, whereas SGD has a constant cost $O(1)$.
 We see that even though each iteration of SGD is $n$ times more efficient than an iteration of full batch gradient descent, its overall convergence rate is $O(\inv{t})$, far worse than the linear rate of gradient descent $O(e^{-\frac{t}{\kappa}})$. 
Finding an algorithm with a cheap $O(1)$ iteration cost and a linear convergence rate seemed impossible until the advent of SAG \citep{roux2012stochastic} and variance reduction techniques. 

\subsection{Variance Reduction}
\label{ssec:variance-reduction}
Compared to the expected population risk \cref{eq:population-risk}, the empirical risk \cref{eq:finite-sum} has a special finite sum structure which is not exploited by SGD.
Tapping into this structure, the breakthrough work of \citet{roux2012stochastic} designed and analyzed a stochastic algorithm with a  cheap $O(1)$ iteration cost and a linear convergence rate. 
This algorithm is called stochastic averaged gradient or SAG. 
Similar to SGD, at each step it sample a datapoint $x_i$ computes its gradient $\nabla f(\theta_{t}, x_i)$.
The difference is that it estimates the true gradient thanks to past gradients of each individual data points $\nabla f(\theta_{t_i}, x_i)$ where $t_i$ is the last time that we sampled $x_i$.
Finally the update of SAG writes
\alignn{
	\theta_{t+1} = \theta_t - \frac{\lr_t}{n} \sum_i \nabla f(\theta_{t_i}, x_i) \;.
	\tag{SAG}
}
Following this path, \citet{defazio2014saga} introduced SAGA a very similar algorithm with an unbiased gradient estimate  allowing for a simpler analysis.
Concurrently \citet{shalev-shwartz_stochastic_2013} analyzed SDCA, an algorithm maximizing the dual formulation (see \S\ref{ssec:fenchel-duality}), which enjoys the same cheap update cost and linear convergence rate.
In our first contribution, we improve upon SDCA and apply it to the challenging problem of conditional random fields (see \S\ref{ssec:PGM}).

Unfortunately, the memory footprint of SAG, SAGA or SDCA is $O(n d)$, which can quickly become prohibitive for large datasets or large models. 
\citet{johnson2013accelerating} introduced stochastic variance reduced gradient (SVRG) to alleviate this issue. 
Instead of storing all past gradients, SVRG stores one past iterate $\theta_T$ along with its full batch gradient $\nabla F(\theta_T)$, and it applies the update
\alignn{
	\theta_{t+1} = \theta_t - \lr_t (\nabla f(\theta_t) - \nabla f(\theta_T) + \nabla F(\theta_T))  \;.
	\tag{SVRG}
}
Thus SVRG only needs $O(d)$ memory, but it needs twice more compute than plain SGD. As such it is the variance reduction technique that is most amenable to the optimization of large models such as neural networks.

\subsection{Fenchel Duality}
\label{ssec:fenchel-duality}
As previously mentioned, and fully explained in our first contribution, SDCA operates on the dual formulation of 
\alignn{
	\min_\theta \inv{n} \sum_i f(y_i, \theta^\top x_i) + \frac{\lambda}{2} \norm{\theta}^2 \;.
}
But what is this dual formulation ?

\paragraph{Convex Conjugates.}
To explain Fenchel duality properly, we need to introduce the convex conjugate of a function.
\begin{definition}[convex conjugate]
	The convex conjugate of a function $f:\real^d \rightarrow \real\cup{+\infty}$ is defined by the pointwise formula
	\alignn{
		f^*(y) := \max_x \lin{y, x} - f(x)
	}
\end{definition}
This transformation is a ubiquitous concept throughout Science.
 It appears in thermodynamics and classical mechanics as the Legendre transform (a special case), in convex optimization and machine learning as the Fenchel conjugate or the convex conjugate. 
At first sight this definition seems arbitrary, but it admits geometrical interpretations along with many properties that make it a useful tool. 
The author of this thesis produced several interactive tools to grasp a better understanding of convex conjugates : \href{https://remilepriol.github.io/dualityviz/}{DualityViz} and \href{https://remilepriol.github.io/dualityviz/dual_snakes.html}{Dual Snakes}.

One of the most interesting properties of convex conjugation is that for convex functions, the convex conjugate of the conjugate is equal to the function itself, e.g.
\alignn{f^{**} = f \; .}

\paragraph{Fenchel Dual.}
Assume we want to solve a composite minimization problem
\alignn{
	\min_x f(x) + g(\mA x)
	\label{eq:composite-minimization}
}
where $f:\cX \rightarrow\real$ and $g:\cY \rightarrow \real$ are convex functions and $\mA: \cX \rightarrow \cY$ is a linear operator.
Under mild assumptions, this problem can be equivalently expressed with the convex conjugates of $f$ and $g$
\alignn{
	\min_x f(x) + g(\mA x)
	&= \min_x \max_y f(x) + \lin{Ax, y} - g^*(y) \\
	&\geq \max_y \min_x f(x) + \lin{x, A^\top y} - g^*(y) \\
	&= \max_y - f^*(-A^\top y) - g^*(y) \; .
}
This last line is known as the Fenchel dual of problem~\eqref{eq:composite-minimization}. 
To reach it, we inverted min and max between the first and second line. 
Fenchel's duality theorem states sufficient conditions for this inequality to be an equality, in which case we say that strong duality holds.
Fenchel duality is equivalent to Lagrange duality, but Fenchel's is more convenient for unconstrained problems or problems where the constraints are implicitly defined in the objective, whereas Lagrange's is more convenient for explicitly defined constraints.

SDCA, as well as many other optimization algorithms, directly store and update the dual variable $y$. 
It is well defined for generalized linear models, e.g. models defined with the exponential family. 

\section{Probabilistic Models}
\subsection{Exponential Families}
\label{ssec:exponential-families}
Exponential families are among the simplest parametric models of distributions.
To define an exponential family, take a variable $x$ in $\cX$ equipped with the base measure $\nu$. 
Then extract a sufficient statistic $T(x) \in \real^d$.  
Then take the inner product between some parameter $\nat$ and $T(x)$. 
This inner product may be negative, so to ensure it is positive, take its exponential $e^{ \lin{\E[T(x)] , \nat} }$.
The mass (for discrete random variables) or the density (for continuous random variables) with respect to $\nu$ is then defined to be proportional to this exponential
\alignn{
	\pmodel(x) \propto e^{ \lin{\E[T(x)] , \nat} } \nu(x) \; .
}
The logarithm of the normalization constant is known as the log-partition function
\alignn{
    A(\nat) := \log \int e^{\langle \nat, T(x) \rangle} \nu(dx) \;.
}
The equation for the negative log-likelihood finally writes
\alignn{
f(\nat) := \E[-\log p_\nat(X)] = A(\nat) - \lin{\E[T(X)] , \nat} \; .
}
Remark that $f$ is convex, and it can simply be seen as a linear modification of the log-partition function $A$ which contains all the complexity.
We provide more properties of these families in our third contribution.

We can always define $z=T(x)$, with $\mu$ the proper push forward modification of $\nu$, in which case we say that $Z$ belongs to the \emph{natural exponential family} on $\mu$.

Most common parametric distributions are exponential families :
Categorical, Gaussians, Gamma, Wishart, Dirichlet, etc...
A remarkable exception is the non-central Laplace $p(x) \propto e^{\abs{x-\mu}}$ which cannot be expressed in this form.


\paragraph{GLM.}
Generalized linear models (GLM) are a powerful tool in supervised learning. 
Taking features $x$ and labels $y$, a GLM models the conditional distribution $\ptrue(y \cond x)$ with an exponential family whose parameter is a linear function of $x$. 
For simplicity, we consider the natural exponential family $T(y) = y$.
The model writes
\alignn{
	\pmodel ( y \cond x) = \exp( y^\top \vtheta x - A(\vtheta x) )
}
where $\vtheta$ is a matrix of size $\dim(y) \times \dim(x)$.

In our first contribution, we study an algorithm for training a GLM for categorical distributions, e.g. logistic regression, with a number of categories growing exponentially with the input size.
For this purpose we use independence assumptions that are formalized by probabilistic graphical models. 

\subsection{Probabilistic Graphical Models}
\label{ssec:PGM}

One the most useful properties we can model about the natural distribution $\ptrue(x)$ is the notion of (conditional) independence between variables. For instance, in a simple video game, two stacks of frames are often independent given the stack of frames in between them. This kind of independence statements can be specified with graphs thanks to probabilistic graphical models -- see \citet{pearl1988probabilistic} for an historical reference, or \citet{wainwright2008graphical} or  \citet{koller2009PGM} for a more recent review.

We start by presenting undirected graphical models.
GLMs associated to undirected graphical models are known as conditional random field (CRF), and they are at the core of our first contribution.
Then we introduce directed graphical models, which are necessary to understand structural causal models and our second contribution. 

\subsubsection{Undirected Graphical Models, a.k.a. Markov Random Fields}
Let $\cG$ be an undirected graph defined by 
its vertices $\cV = \bracket{1,\dots,d}$ 
and its edges $(i, j) \in \cE$.
\begin{definition}[clique]
	The set $C = \bracket{v_1, \dots, v_k}$ is said to be 
	a clique of $\cG$ if and only if it forms 
	a complete graph, e.g. 
	$\forall i \neq j, (v_i, v_j) \in \cE$.
\end{definition}
\begin{definition}[maximal clique]
	A clique $C$ is maximal if it is not contained in any clique, e.g.
	$\forall C', (C\subset C' \implies C'$ is not a clique$)$.
\end{definition}
We name $\cC$ the set of maximal cliques of $\cG$.
We are now ready to define the independence statement.
\begin{definition}
	A distribution $\ptrue$ is said to factor along $\cG$ if and only if its density verifies
\alignn{
	\ptrue(x) = \prod_{C\in \cC} \psi_C (x_C)
}
where $x_C$ is a vector containing the rows of $x$ indexed by $i \in C$, and $\psi_C : \real^{\abs{C}} \rightarrow \real$ are real-valued functions of $\abs{C}$ elements.
We refer to $\psi_C$ as the potential of the clique $C$.
\end{definition}


\paragraph{Exponential Graphical Model.}
If $\pmodel$ belongs to the exponential family, then a sufficient condition for $\pmodel$  to factor along $\cG$ is for its sufficient statistic to decompose along the cliques of $\cG$, e.g.
\alignn{
	T(x) &= \sum_{C\in\cC} T_C(x_C) \\
	\implies 
	\pmodel(x) &\propto \prod_{C\in \cC} e^{\lin{T_C(x_C) , \nat}} \; .
}
We exploit this fact in our first contribution.


\subsubsection{Directed Graphical Models, a.k.a. Bayesian Networks} 
Directed probabilistic graphical models are also known as Bayesian Networks since \citet{pearl1985bayesian} coined this term.
They are perhaps simpler to understand than undirected graphical models, but they are not easier to deal with.

Suppose we observe a random variable $X=(X_1, \dots, X_d) \in \real^d$ with probability law $\ptrue(X)$. 
We are also given a Directed Acyclic Graph (DAG) $\cG$ with vertex $\cV=\{1, \dots, d \}$ and edges $\cE$ . 
We denote $\parents(i)$ the parents of node $i$. 
This is the empty set if $i$ has no parents. 
\begin{definition}
We say that $\ptrue$ factorizes along $\cG$ iff
\begin{align}
    \ptrue(X) = \prod_{i=1}^d \ptrue(X_i | X_{\parents(i)}) \; .
\end{align}
\end{definition}
In words, the only conditional dependencies of $\ptrue$ are indicated by the edges of the graph $G$. The less edges in $G$, the more we know about $X$. In fact if we know nothing about $\ptrue$, we still know that we can write it as 
\begin{align}
    \ptrue(X) = \prod_{i=1}^d \ptrue(X_i | X_{<i} )
\end{align}
by definition of conditional probability -- modulo some positivity constraints. Consequently, a useful graph should have only a few edges, or equivalently a low degree.

\paragraph{Structure Learning.}
In unsupervised learning, either we posit that the data factorizes along a graph and exploit this information to learn a density model $\pmodel$ with less data. Either we set the goal of discovering these conditional independence structures. This goal is known as structure learning. Current solutions to this problem fall into two categories
\begin{enumerate}
    \item Explicitly find out conditional independences with statistical testing, and build the graph from there.
    \item Use a scoring function to explore all possible graphs and keep the one with the highest score. The scoring function is often designed as the posterior probability of the structure given the data.
\end{enumerate}

Directed grahical models have proven useful in many modeling areas. However they alone are not able to predict what will happen if one of the variables is affected by some external stimuli. This is the topic of causal inference.

\subsection{Causal Inference}
\label{ssec:SCM}

Causal inference use directed graphical models to predict the effect of interventions in the world. We are now going to introduce 2 key elements of this theory: do-calculus and structural causal models.

\subsubsection{Do-calculus}
Assume we have data for kidney stone treatments performed in one hospital. 
For each patient, we know the treatment they received $X$, the outcome $Y$ -- did they successfully heal? -- and the size of the stones they found during the surgery $Z$. 
A new patient arrive. We have to recommend the treatment that will maximize their chance of recovery.  
How should we process the data to take this decision ?

By now you may have recognized this classic story that is an instance of Simpson's paradox. 
In this example, the straightforward solution would be to recommend the treatment with the highest success rate. 
But it so happens that the treatment received by past patients was picked based on their symptoms, which were themselves a function of the stone sizes. 
In this example, the stone size is a \textit{confounder} that affects both the treatment and the outcome. 
One should first partition based on $Z$ the data before aggregating the success rates. But why is that and how to formalize that ? The answer lies in the work of Judea Pearl \citep{pearl2009causality} and other statisticians. It can be formalized with the help of graphical models such as Figure \ref{fig:simpsons_paradox}.

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{scope}[every node/.style={circle,thick,draw}]
        \node (X) at (-2,0) {$X$};
        \node (Y) at (2,0) {$Y$};
        \node (Z) at (0,3) {$Z$};
    \end{scope}
    
    \begin{scope}[>={Stealth[black]},
                  every edge/.style={draw=black,very thick}]
        \path [->] (X) edge (Y);
        \path [->] (Z) edge (Y);
        \path [->] (Z) edge (X);
    \end{scope}
    \end{tikzpicture}
    \caption{The graph of causal relationships between treatments $X$, outcome $Y$ and stone size $Z$. $Z$ is a cause of both $X$ and $Y$, which makes it a confounder.}
    \label{fig:simpsons_paradox}
\end{figure}

The question we asked is an \textit{interventional question}: what will happen \textit{if we assign $X=x$?} This action effectively removes the observed statistical dependency between $X$ and $Z$. The outcome distribution of this action should not be computed as the simple conditional probability $P(Y|X=x)$, but as another quantity that we will denote $P(Y| \Do(x))$. The gold standard to estimate this quantity would be to perform a \textit{randomized control trial}, where we blindly and randomly assign treatments to incoming patient, then observe and report success rate. But we want to exploit the data we already observed to estimate this quantity. That is where the \textit{do-calculus} comes into play. It is a set of rules based on graphs that transforms do-statements such as $P(Y| \Do(x))$ into an equation written in terms of observed probabilities. 
In the kidney stone example, we can estimate $P(Y| \Do(x))$ from observational data thanks to the \textit{backdoor adjustment formula}
\begin{align}
    P(Y|\Do(x)) = \sum_z P(Y | x, z) P(z) 
    \neq \sum_z P(Y | x, z) P(z|x) = P(Y | x) \; .
    \label{eq:backdoor_adjustment}
\end{align}

What is critical here is that $Z$ is a cause of $X$. If instead $X$ caused $Z$ then causal effect and conditional would be equal $P(Y|\Do(x)) = P(Y | x)$. Yet from a Bayesian network perspective, both arrow directions make a complete graph, which encode the same absence of conditional independence. In other words, a causal graphical model encodes strictly more information than a Bayesian network.

The backdoor adjustment formula is the most famous instance of do-calculus, but more complex rules exist for complex graph with both observed and unobserved variables. Quite recently, \citet{huang2012pearl} proved that these rules are complete, meaning that if a do-statement can be expressed in terms of observed probabilities, then one will be able to find the right formula by applying these rules. 


\subsubsection{Structural Causal Models}

Thanks to the rules of do-calculus, knowing the causal graph can be very useful. So far we have talked about this in a non-parametric setting, assuming we have direct access the observed conditional probabilities $P(Y|X, Z)$. In reality we need to parametrize these mechanisms. This is what a Structural Causal Model (SCM) is for. It describes a causal model by a set of unobserved independent exogenous noise variables $U_1, \dots, U_d$, and a set of functions $f_1, \dots, f_d$ such that
\begin{align}
    X_i = f_i(X_{\parents(i)}, U_i), \forall i \;.
    \label{eq:scm}
\end{align}
Given a DAG $\cG$, these functions, and distributions for the exogenous noise, one can sample a vector $X$ by sampling the noises and applying these functions in a topological order of $\cG$.

Among other things, SCMs are useful to answer \textit{counterfactual questions}: what would have happened if I had given the other treatment to this patient ?  Counterfactuals are a major topic in the causality community, but they not relevant to this thesis so we will not cover this theory.

While the formalism of \eqref{eq:scm} may seem trivial at first, it becomes useful when one starts thinking about causal structure discovery. 
If we assume a parametric form for the functions, then the graphical structure can become identifiable, meaning that only one graph could have generated the observed data. One such example is if we assume $f_i$ are linear and noises are non-Gaussian. 
However the interest of these identifiability results is limited because in general, we have no guarantee on the shape of the function that generated the data. 

The SCM formalism enables us to think about a much deeper hypothesis: \textbf{Independent Causal Mechanisms}. This hypothesis postulates that knowing something about one mechanism does not provide any information about the others. This can be formalized by various means. One of them is Algorithmic Information Theory: the Kolmogorov Complexity of the set $\{ f_1, \dots, f_d \}$
\footnote{We are not including the exogenous noise distributions for simplicity.}
is on the same order of magnitude as the sum of the Kolmogorov Complexity of each function taken independently. 
Using this independence insight, one can devise algorithms that aims at finding the causal structure of the data. See \citet{peters2017elements} for a book on this topic.

Our second contribution addresses an idea to discover causal structure from interventional data, e.g. data coming from (possibly unknown) interventions. 
We have now provided all the key elements to understand this thesis. 
Let us emphasize that causal inference and causal discovery are taking more and more space in machine learning. 
We refer the reader to \citep{scholkopf2019causality} for a modern review of the literature.

