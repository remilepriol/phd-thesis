\documentclass[12pt]{report} % nottoc,numbib
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}
\usepackage[french,english]{babel}
\selectlanguage{english}

% Speed up compilation by focusing on one part at a time
%\includeonly{introduction,discussion}
%force-texpad-dependency: discussion.tex
%force-texpad-dependency: introduction.tex
%force-texpad-dependency: contrib1.tex
%force-texpad-dependency: contrib2.tex
%force-texpad-dependency: contrib3.tex

\usepackage{silence}
% Disable all warnings issued by latex starting with "You have..."
\WarningFilter{latex}{You have requested package}
\WarningFilter{latex}{No positions in optional float specifier.}
\WarningFilter{latexfont}{}

\usepackage{style/math_commands}

\usepackage{style/thesis_style}
\renewcommand{\sidebarname}{Sidebar}

\usepackage[]{setspace} % should use option onehalfspacing but ugly
\usepackage{style/coverpage}

\usepackage[round]{natbib} % option sectionbib for biblio in each chapter
%\usepackage{chapterbib}  

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{microtype}       % set of techniques to improve general appearance
\usepackage{graphicx}
\graphicspath{
	{figs/causal-optimization/}
	{figs/MAP-convergence/}
	{figs/sdca4crf/}	
}
\usepackage{wrapfig}
%\usepackage{subfigure}
%\usepackage{subcaption}
%\usepackage{placeins}
\usepackage{tikz}           % define for loops for placing figures
\usetikzlibrary{arrows,arrows.meta}  % special arrow heads
\usepackage[super]{nth} % 1st 2nd etc...
\usepackage{paralist} % for compact enumerate environment
\usepackage{algorithm}  % float wrapper for algorithms
\usepackage[noend]{algorithmic}   % algorithm typesetting environment
\usepackage[all]{xy}  % xy diagrams
\usepackage{graphicx}
\usepackage{subcaption}  % write captions for subfigures
\usepackage{adjustbox}  % scale properly boxed content
\usepackage{multicol}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage{pgf}
\pgfkeys{/pgf/number format/.cd ,precision=1,sci generic={exponent={\times 10^{#1}}}}
\newcommand\convert[1]{\pgfmathprintnumber{#1}}


%% <lmodern> incorpore les fontes en T1, pour
%% faciliter le dépôt final. Ceci n'est pas la
%% seule option :
%%  1. Si cm-super est installé, vous pouvez enlever <lmodern>
%%     (à ce moment, la police est un peu plus fidèle
%%      au Computer Modern orginal);
%%  2. Si vous avez une police préférée, par exemple,
%%     <times> ou <euler> ou <mathpazo> (et bien d'autres),
%%     alors vous pouvez remplacer <lmodern> ci-bas.
%% Par contre, si vous faîtes face à un problème d'encapsulation
%% lors dépôt final, il se peut que la solution soit d'utiliser <lmodern>.
%% (Parfois le problème est au niveau de l'installation, donc
%%  essayez de compiler sur un autre ordinateur sur lequel vous êtes
%%  certain·e que l'installation est bonne.)
\usepackage{lmodern}

%% Il n'est pas nécessaire d'utiliser <babel>, car
%% les commandes intégrées par la classe <dms>
%% \francais et \anglais font le travail. Néanmoins,
%% certains autres packages nécessitent <babel> (comme
%% <natbib>), donc simplement enlever les % devant <babel>
%% dans ce cas. Attention! Certains packages sont sensibles
%% à l'ordre dans lequel ils sont chargés.
% \francais % or
% \anglais
%% La commande \sloppy peut avoir des effets étranges sur les
%% lignes de certains paragraphes.  Dans ce cas, essayez \fussy
%% qui suppresse les effets de \sloppy.
%% (\fussy est normalement le comportement par défaut.)
%% On redéfinit \sloppy, pour tenter de réduire les comportements
%% étranges. Le seul changement apporté à la version originale
%% est la valeur de \tolerance.
\def\sloppy{%
  \tolerance 500%  %9999 dans LaTeX ordinaire, mauvaise idée.
  \emergencystretch 3em%
  \hfuzz .5pt
  \vfuzz\hfuzz}
\sloppy   %appel de \sloppy pour le document
%%\fussy  %ou \fussy


%% pour que la largeur de la légende des figures soit = \textwidth
\usepackage[labelfont=bf, width=\linewidth]{caption}

%\usepackage[dvipsnames]{xcolor} % already loaded in thesis_style
\definecolor{blue}{rgb}{0,0.08,0.45}
%% <hyperref> et <bookmark> devraient être les dernier package a être chargé,
%% donc chargez vos packages avant.
\usepackage{hyperref}       % hyperlinks
\hypersetup{ % SLJ: my standard paper setup...
  	pdftitle = {Optimization tools for non-asymptotic statistics in the exponential families},
  	pdfauthor = {Rémi Le Priol},
  	pdfsubject = {machine learning, exponential family, Bregman divergence, non-asymptotic statistics, sample complexity, duality, stochastic optimization, variance reduction, structured prediction, causality.},
  	pdfkeywords = {Machine learning, exponential family, Bregman divergence, non-asymptotic statistics, sample complexity, duality, stochastic optimization, variance reduction, structured prediction, causality.},
	pdfborder=0 0 0,
	pdfpagemode=UseNone,
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue,
	filecolor=blue,
	urlcolor=blue,
	pdfview=FitH,
%	draft,
	final,
}
\usepackage{hypcap}   % Corrige la position du lien pour les images
\usepackage{bookmark} % Remédie à des petits de <hyperref> 
			%(important qu'il apparaisse APRÈS <hyperref>)

\usepackage[capitalise]{cleveref}
\newcommand{\rlp}[1]{\textcolor{BrickRed}{(RLP:#1)}}
\newcommand{\TODO}[1]{\textcolor{cyan}{(TODO #1)}}
\newcommand{\tocite}{\textcolor{purple}{(add citation)}}


%% Numérotation des équations par section
%% et des  tableaux et figures par chapitre.
%% Ceci peut être modifié selon les préférences de l'utilisateur.
\numberwithin{equation}{chapter}
\numberwithin{table}{chapter}
\numberwithin{figure}{chapter}
\usepackage{thm-restate}

%% Si on veut faire un index, il faut décommenter la ligne
%% suivante. Ajouter des mots à l'index avec la commande \index{mot cle} au
%% fur et à mesure dans le texte.  Compiler, puis taper la commande
%% makeindex pour creer les indexs.  Après une nouvelle compilation,
%% vous aurez votre index.
%%\makeindex
 % some custom math commands
 
\newcommand*{\ptrue}{\ensuremath{\bm{p}}}
\newcommand{\nat}{\theta}
\newcommand*{\pmodel}{\ensuremath{\bm{p}_{\theta}}}
\newcommand*{\lr}{\gamma}
\newcommand*{\loss}{\ell}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%                                     %%%%%%%%%%%%%
%%%%%%%%%% D é b u t    d u    d o c u m e n t %%%%%%%%%%%%%
%%%%%%%%%%                                     %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%------------------------------------------------- %
%%              pages i et ii                       %
%%------------------------------------------------- %

%%% Front matter is typeset using roman page numbers
\pagenumbering{roman}


\Auteur{Rémi}{Le Priol}
\President{}{Ioannis}{Mitliagkas}{}
\Directeur{}{Simon}{Lacoste-Julien}{}
\CoDirecteur{}{Yoshua}{Bengio}{}         % s'il y a lieu
%%\codirecteurs{Nom du 2e codirecteur}         % s'il y a lieu
\Membres{1}{}{Guillaume}{Rabusseau}{}  
\Membres{2}{}{Nicolas}{Flammarion}{}  

%titre: 15 mots, max. 175 caracteres
\PhD{Optimization Tools for Non-Asymptotic Statistics in Exponential Families}
    {}
    {d'informatique et de recherche op\'{e}rationnelle}
    {informatique}
    {Décembre}
    {2021}

\PagesCouverture
\newpage
%%------------------------------------------------- %
%%              pages iii                           %
%%------------------------------------------------- %
 \begin{flushright} 
 \emph{Les théorèmes sont démontrés par ceux qui y croient.}\\~\\
 \emph{Theorems are proved by those who believe in them.}  \\~\\
André Weil 
 \end{flushright}
 
% \vspace{4cm}
 
% \emph{Le débutant a l'impression que les autres savent tout. Il ne comprend pas comment avec les mêmes matériaux, avec les mêmes outils, il trouverait quelque chose que les autres ne savent pas faire.}\\~\\
%  \emph{The beginner is under the impression that others know everything. He does not understand how, with the same material, with the same tools, he could find something that others cannot do.}\\ ~\\
% François Loeser

\chapter*{Résumé}
Les familles exponentielles est une classe de modèles omniprésente en statistique.
D'une part, elle  peut modéliser n'importe quel type de données. 
En fait la plupart des distributions communes en font partie : Gaussiennes, variables catégoriques, Poisson, Gamma, Wishart, Dirichlet. 
D'autre part elle est à la base des modèles linéaires généralisés (GLM), une classe de modèles fondamentale en machine learning. 
Enfin les mathématiques qui les sous-tendent sont souvent magnifiques, grâce à leur lien avec la dualité convexe et la transformée de Laplace.
L'auteur de cette thèse a fréquemment été motivé par cette beauté.
Dans cette thèse, nous faisons trois contributions à l'intersection de l'optimisation et des statistiques, qui tournent toutes autour de la famille exponentielle. 

La première contribution adapte et améliore un algorithme d'optimisation à variance réduite appelé ascension des coordonnées duales stochastique (SDCA), pour entraîner une classe particulière de GLM appelée champ aléatoire conditionnel (CRF). Les CRF sont un des piliers de la prédiction structurée. Les CRF étaient connus pour être difficiles à entraîner jusqu'à la découverte des technique d'optimisation à variance réduite. Notre version améliorée de SDCA  obtient des performances favorables comparées à l'état de l'art antérieur et actuel. 

La deuxième contribution s'intéresse à la découverte causale. 
Les familles exponentielles sont fréquemment utilisées dans les modèles graphiques, et en particulier dans les modèles graphique causaux. 
Cette contribution mène l'enquête sur une conjecture spécifique qui a attiré l'attention dans de précédents travaux : les modèles causaux s'adaptent plus rapidement aux perturbations de l'environnement. 
Nos résultats, obtenus à partir de théorèmes d'optimisation, soutiennent cette hypothèse sous certaines conditions. Mais sous d'autre conditions, nos résultats contredisent cette hypothèse . Cela appelle à une précision de cette hypothèse, ou à une sophistication de notre notion de modèle causal.

La troisième contribution s'intéresse à une propriété fondamentale des familles exponentielles. 
L'une des propriétés les plus séduisantes des familles exponentielles est la forme close de l'estimateur du maximum de vraisemblance (MLE), ou maximum a posteriori (MAP) pour un choix naturel de prior conjugué. 
Ces deux estimateurs sont utilisés presque partout, souvent sans même y penser. 
(Combien de fois calcule-t-on une moyenne et une variance pour des données en cloche sans penser au modèle Gaussien sous-jacent ?)
Pourtant la littérature actuelle manque de résultats sur la convergence de ces modèles pour des tailles d'échantillons finis, lorsque l'on mesure la qualité de ces modèles avec la divergence  de Kullback-Leibler (KL). 
Pourtant cette divergence est la mesure de différence standard en théorie de l'information.
En établissant un parallèle avec l'optimisation, nous faisons quelques pas vers un tel résultat, et nous relevons quelques directions pouvant mener à des progrès, tant en statistiques qu'en optimisation.

Ces trois contributions mettent des outil d'optimisation au service des statistiques dans les familles exponentielles : améliorer la vitesse d'apprentissage de GLM de prédiction structurée, caractériser la vitesse d'adaptation de modèles causaux, estimer la vitesse d'apprentissage de modèles omniprésents. 
En traçant des ponts entre statistiques et optimisation, cette thèse fait progresser notre maîtrise de méthodes fondamentales d'apprentissage automatique.

\subsection*{Mots-clés}
Apprentissage automatique, famille exponentielle, divergence de Bregman, statistiques non-asymptotiques, taux de convergence, dualité, optimisation stochastique, réduction de variance, prédiction structurée, causalité.

%%------------------------------------------------- %
%%              pages iv                            %
%%------------------------------------------------- %

% \anglais
\chapter*{Abstract}

Exponential families are a ubiquitous class of models in statistics. 
On the one hand, they can model any data type.
Actually most common distributions are exponential families: Gaussians, categorical, Poisson, Gamma, Wishart, or Dirichlet. 
On the other hand, they sit at the core of generalized linear models (GLM), a foundational class of models in machine learning. 
They are also supported by beautiful mathematics thanks to their connection with convex duality and the Laplace transform.
This beauty is definitely responsible of the existence of this thesis.
In this thesis, we make three contributions at the intersection of optimization and statistics, all revolving around exponential families. 

The first contribution adapts and improves a variance reduction optimization algorithm called stochastic dual coordinate ascent (SDCA) to train a particular class of GLM called conditional random fields (CRF). CRF are one of the cornerstones of structured prediction. CRF were notoriously hard to train until the advent of variance reduction techniques, and our improved version of SDCA performs favorably compared to previous state-of-the-art.  

The second contribution focuses on causal discovery. 
Exponential families are widely used in graphical models, and in particular causal graphical models. 
This contribution investigates a specific conjecture that gained some traction in previous work : causal models adapt faster to perturbations of the environment. `
Using results from optimization, we find strong support for this assumption when the perturbation is coming from an intervention on a cause, and support against this assumption when perturbation is coming from an intervention on an effect.
 These pieces of evidence are calling for a refinement of  the conjecture.

The third contribution addresses a fundamental property of exponential families. 
One of the most appealing properties of exponential families is its closed form maximum likelihood estimate (MLE) and maximum a posteriori (MAP) for a natural choice of conjugate prior. These two estimators are used almost everywhere, often unknowingly
-- how often are mean and variance computed for bell shaped data without thinking about the Gaussian model they underly ?
Yet literature to date is lacking results on the finite sample convergence property of the information (Kulback-Leibler) divergence  between these estimators and the true distribution. 
Drawing on a parallel with optimization, we take some steps towards such a result, and we highlight directions for progress both in statistics and optimization. 

These three contributions are all using tools from optimization at the service of statistics in exponential families : improving upon an algorithm to learn GLM, characterizing the adaptation speed of causal models, estimating the learning speed of ubiquitous models.
By tying together optimization and statistics, this thesis is taking a step towards a better understanding of fundamentals of machine learning.

\subsection*{Keywords}
Machine learning, exponential family, Bregman divergence, non-asymptotic statistics, sample complexity, duality, stochastic optimization, variance reduction, structured prediction, causality.


%%------------------------------------------------- %
%%        page v --- Table de matieres              %
%%------------------------------------------------- %

 % \cleardoublepage termine la page actuel et force TeX
 % a poussé les éléments flottant (fig., tables, etc.) sur
 % la page (normalement TeX les garde en suspend jusqu'à ce
 % qu'il trouve un endroit approprié).
%% TABLE DES MATIÈRES
\cleardoublepage
\pdfbookmark[chapter]{\contentsname}{toc}  % Crée un bouton sur
                                           % la bar de navigation
\tableofcontents
 % LISTE DES TABLES
\cleardoublepage
\phantomsection  % Crée une section invisible (utile pour les hyperliens)
\listoftables
 % LISTE DES FIGURES
\cleardoublepage
\phantomsection
\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LISTE DES SIGLES ET ABRÉVIATION %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{List of acronyms and abbreviations}
\begin{tabular}{lll}
  AISTATS & conference on Artificial Intelligence and STATisticS \\
  ASG & Average Stochastic Gradient \\
  BCFW & Block-Coordinate Frank-Wolfe \\
  CONLL & conference on COmputational Natural Language Learning \\
  CPU & Central Processing Unit \\
  CRF & Conditional Random Field \\
  DAG & Directed Acyclic Graph \\
  e.g. & \emph{exempli gratia} [for instance]\\
  ERM & Empirical Risk Minimization \\
  GD & Gradient Descent \\
  GLM & Generalized Linear Model \\
  GPU & Graphical Processing Unit \\
  i.e.& \emph{ide est} [that is]\\
  KL & Kullback-Leibler divergence \\
  MAP & Maximum A Posteriori estimate \\
  MD & Mirror Descent \\
  ML & Machine Learning \\
  MLE & Maximum Likelihood Estimate \\
  NER & Named-Entity Recognition \\
  NUS & Non-Uniform Sampling \\
  OCR & Optical Character Recognition \\
  OEG & Online Exponentiated Gradient \\
  POS & Part Of Speech tagging \\
  RAM & Random Access Memory \\
  resp. & respectively \\
  SAG & Stochastic Average Gradient \\
  SCM & Structural Causal Model \\
  SDCA & Stochastic Dual Coordinate Ascent \\
  SGD & Stochastic Gradient Descent \\  
  SMD & Stochastic Mirror Descent\\
  SVRG & Stochastic Variance Reduced Gradient \\
  SVM & Support Vector Machine\\
  UAI & conference on Uncertainty in Artificial Intelligence \\
\end{tabular}

%%------------------------------------------------- %
%%              pages vi                            %
%%------------------------------------------------- %

\chapter*{Remerciements}
% \begin{minipage}{.7\linewidth}

À ma mère et mon père pour le soutien qu'ils m'ont toujours offert.
À mes deux frères pour la continuité, la présence et les ouvertures qu'ils apportent à ma vie.

Merci Simon pour ces presque 5 années de collaborations,
pour ton soutien à travers les succès et les doutes,
pour ta bienveillance et ton écoute face à mes choix de vie, qui m'ont entre autres mené à rentrer en France pris par un sentiment d'urgence écologique.

Merci Yoshua d'avoir posé les pierres du Mila, et d'y défendre, une recherche ouverte et créative.
Ce laboratoire qui a été pour moi à la fois une maison et un lieu d'aventures.

Merci Gabriel d'avoir ouvert la voie à Montréal (ton feu y a fait fondre la neige).

Merci Romain Lopez. Sans toi je n'aurais peut-être pas envoyé le mail qui m'a amené au Mila.

Pour tous mes collaborateurs et amis de Montréal et d'ailleurs, dans l'ordre de ma mémoire:
Thomas, Akram, Gauthier, Ahmed, Sébastien, Tristan, Waïss, Reza, Radu, Hadrien,  Damien, Frederik, Tom, Hugo, Falco, Ju, Louve, Robin, Nicole, Thiago, Oscar, Léonard, le Black Yak, Jill, Heidi, Byron, Joe, Simon, Aristide, Deanna, Alejandro the monk, Anna.

Pour ceux qui m'aident aujourd'hui à me réinventer : Lise Dargentolle, X-Urgence Écologique, Vincent, Hervé et les châteaux d'acroyoga.

Et surtout merci à Aurélie de m'aider à écrire ces pages et à écrire ma vie.

 %
 % Fin des pages liminaires.  À partir d'ici, les
 % premières pages des chapitres ne doivent pas
 % être numérotées
 %
% \NoChapterPageNumber
\cleardoublepage

%\chapter*{Notation}
%
%\begin{tabular}{lll}
%  The set of real numbers \dotfill & $\real$ \\
%  Scalars are lower-case letters \dotfill & $\lambda$       \\
%  Vectors are lower-case bold letters  \dotfill & $\vtheta$\\
%  Matrices are upper-case bold letters\dotfill &  $\mA$ \\
% \end{tabular} 

%$\simplex_K$ to denote the K-simplex
%the indicator function $\ones\{ b\}$
%\newpage

\pagenumbering{arabic}
\setcounter{page}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%                           %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%  I N T R O D U C T I O N  %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%                           %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\include{introduction}
\include{contrib1}
\include{contrib2}
\include{contrib3}
\include{discussion}

 % S'il y a une bibliographie pour tout le document, on peut
 % utiliser les commandes suivantes. À noter que le style est
 % laisser au choix de l'auteur·e. (Il est même possible
 % d'utiliser <natbib>).
 % Il est possible d'avoir une bibliographie pour chaque
 % chapitre. Consulter l'article en exemple pour voir
 % comment faire.
%  \chapter*{References}
\bibliographystyle{abbrvnat}
%\bibliography{references}
\bibliography{references1,references2,references3}

\end{document}
